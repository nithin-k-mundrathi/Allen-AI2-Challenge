{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Case_Study_2_BERT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ErE1GlGUp-6",
        "colab_type": "code",
        "outputId": "fe1bcea5-0e8b-4de2-93fa-c31f8522135b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-07 16:37:06--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.204.128, 2404:6800:4008:c04::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.204.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   159MB/s    in 2.5s    \n",
            "\n",
            "2019-07-07 16:37:09 (159 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe-satOJVMrd",
        "colab_type": "code",
        "outputId": "b326178b-33bb-4eb6-dfb8-771a6f60e899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!unzip ./uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnrVufKSYyhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iOR_ST3UJ88",
        "colab_type": "code",
        "outputId": "ee4ae645-416e-4684-87ab-04ff44058c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import csv\n",
        "import os\n",
        "import modeling\n",
        "import optimization\n",
        "import tokenization\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0707 16:38:02.181702 140377027782528 deprecation_wrapper.py:119] From /content/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXIB57r7U62t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None,text_c=None,text_d=None,text_e=None,text_f=None,label=None):\n",
        "\n",
        "    \n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.text_c = text_c\n",
        "    self.text_d = text_d\n",
        "    self.text_e = text_e\n",
        "    self.text_f = text_f\n",
        "    self.label = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP2NU3E-Vlr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PaddingInputExample(object):\n",
        "  \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
        "\n",
        "  When running eval/predict on the TPU, we need to pad the number of examples\n",
        "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
        "  size. The alternative is to drop the last batch, which is bad because it means\n",
        "  the entire output data won't be generated.\n",
        "\n",
        "  We use this class instead of `None` because treating `None` as padding\n",
        "  battches could cause silent errors.\n",
        "  \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EZ4kGdMVov8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               input_ids,\n",
        "               input_mask,\n",
        "               segment_ids,\n",
        "               label_id,\n",
        "               is_real_example=True):\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.segment_ids = segment_ids\n",
        "    self.label_id = label_id\n",
        "    self.is_real_example = is_real_example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoeG0MqZw5cK",
        "colab_type": "code",
        "outputId": "4ed728a9-62d0-41d5-ce37-0a68dad01c02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr7gzuAEYqDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples=[]\n",
        "with open(\"gdrive/My Drive/ARC-V1-Feb2018/ARC-V1-Feb2018-2/ARC-Challenge/ARC-Challenge-Train.jsonl\") as f:\n",
        "      for line in f.readlines():\n",
        "         example = json.loads(line)\n",
        "         examples.append(example)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZlsZ8gjxOnC",
        "colab_type": "code",
        "outputId": "421469e0-b006-4862-bb15-84f600d22a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "examples[1]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'answerKey': 'B',\n",
              " 'id': 'MCAS_2009_5_6516',\n",
              " 'question': {'choices': [{'label': 'A',\n",
              "    'text': 'The refrigerator door is smooth.'},\n",
              "   {'label': 'B', 'text': 'The refrigerator door contains iron.'},\n",
              "   {'label': 'C', 'text': 'The refrigerator door is a good conductor.'},\n",
              "   {'label': 'D', 'text': 'The refrigerator door has electric wires in it.'}],\n",
              "  'stem': 'Which of the following statements best explains why magnets usually stick to a refrigerator door?'}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm5MvRjJ06wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_input(json_inputs):\n",
        "  input_examples=[]\n",
        "  for input_ in json_inputs:\n",
        "    text_a=\"\".join(input_['question']['stem'])\n",
        "    entities=[]\n",
        "    for i in range(0,5):\n",
        "      try:\n",
        "        entities.append(input_['question']['choices'][i]['text'])\n",
        "      except IndexError:\n",
        "        entities.append(\"no option\")\n",
        "        \n",
        "    label=input_['answerKey'] \n",
        "    if(label =='1'):\n",
        "      label='A'\n",
        "    if(label == '2'):\n",
        "      label = 'B'\n",
        "    if(label=='3'):\n",
        "      label = 'C'\n",
        "    if(label=='4'):\n",
        "      label ='D'\n",
        "   \n",
        "    e1=\"\".join(entities[0])\n",
        "    e2=\"\".join(entities[1])\n",
        "    e3 = \"\".join(entities[2])\n",
        "    e4 = \"\".join(entities[3])\n",
        "    e5 = \"\".join(entities[4])\n",
        "    \n",
        "    input_examples.append(InputExample(input_[\"id\"],text_a,e1,e2,e3,e4,e5,label))\n",
        "  return input_examples \n",
        "input_examples=process_input(examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrXlMsiratDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples_cv=[]\n",
        "with open(\"gdrive/My Drive/ARC-V1-Feb2018/ARC-V1-Feb2018-2/ARC-Challenge/ARC-Challenge-Dev.jsonl\") as f:\n",
        "      for line in f.readlines():\n",
        "         example = json.loads(line)\n",
        "         examples_cv.append(example)\n",
        "        \n",
        "input_examples_cv=process_input(examples_cv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVaiLcvBHSpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set_label = []\n",
        "for i in range(22):\n",
        "  set_label.append(input_examples[i].label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV3iAeibHlxR",
        "colab_type": "code",
        "outputId": "4964a30c-0d5d-4fa5-c21b-4225ead6ecd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "set(set_label)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'A', 'B', 'C', 'D'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pR5d4v5u5JW9",
        "colab_type": "code",
        "outputId": "a463ad5c-9c42-47a4-fb38-a923e77171b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "print(input_examples[0].text_a)\n",
        "print(input_examples[0].text_b)\n",
        "print(input_examples[0].text_c)\n",
        "print(input_examples[0].text_d)\n",
        "print(input_examples[0].text_e)\n",
        "print(input_examples[0].text_f)\n",
        "print(input_examples[0].label)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\n",
            "dry palms\n",
            "wet palms\n",
            "palms covered with oil\n",
            "palms covered with lotion\n",
            "no option\n",
            "A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr3HH-EjWbJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, tokens_c,tokens_d,tokens_e,tokens_f,max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)+len(tokens_c) + len(tokens_d) + len(tokens_e) + len(tokens_f)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    \n",
        "    tokens_a.pop()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DPs5LMPZQi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_labels():\n",
        "  return ['A',\\\n",
        " 'B',\\\n",
        " 'C',\\\n",
        " 'D',\\\n",
        " 'E'         \n",
        " ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MvsAuWOWD3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
        "                           tokenizer):\n",
        "  \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
        "\n",
        "  if isinstance(example, PaddingInputExample):\n",
        "    return InputFeatures(\n",
        "        input_ids=[0] * max_seq_length,\n",
        "        input_mask=[0] * max_seq_length,\n",
        "        segment_ids=[0] * max_seq_length,\n",
        "        label_id=0,\n",
        "        is_real_example=False)\n",
        "\n",
        "  label_map = {}\n",
        "  for (i, label) in enumerate(label_list):\n",
        "    label_map[label] = i\n",
        "\n",
        "  tokens_a = tokenizer.tokenize(example.text_a)\n",
        "  tokens_b = None\n",
        "  if example.text_b:\n",
        "    tokens_b = tokenizer.tokenize(example.text_b)\n",
        "    #print(tokens_b)\n",
        "  tokens_c = [\"none\"]\n",
        "  if example.text_c:\n",
        "    tokens_c = tokenizer.tokenize(example.text_c)\n",
        "    #print(tokens_c)\n",
        "  tokens_d  =[\"none\"]\n",
        "  if example.text_d:\n",
        "    tokens_d  = tokenizer.tokenize(example.text_d)\n",
        "    \n",
        "  tokens_e  =[\"none\"]\n",
        "  if example.text_e:\n",
        "    tokens_e  = tokenizer.tokenize(example.text_e) \n",
        "    \n",
        "  tokens_f = [\"none\"] \n",
        "  if example.text_f:\n",
        "    tokens_f = tokenizer.tokenize(example.text_f)\n",
        "    \n",
        "  if tokens_b:\n",
        "\n",
        "    _truncate_seq_pair(tokens_a, tokens_b,tokens_c,tokens_d,tokens_e,tokens_f,max_seq_length - 7)\n",
        "  \n",
        "\n",
        "  tokens = []\n",
        "  segment_ids = []\n",
        "  tokens.append(\"[CLS]\")\n",
        "  segment_ids.append(-1)\n",
        "  for token in tokens_a:\n",
        "    tokens.append(token)\n",
        "    segment_ids.append(-1)\n",
        "  tokens.append(\"[SEP]\")\n",
        "  segment_ids.append(-1)\n",
        "\n",
        "  if tokens_b:\n",
        "    for token in tokens_b:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(0)\n",
        "  if tokens_c:\n",
        "    for token in tokens_c:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(1)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(1)\n",
        "  if tokens_d:\n",
        "    for token in tokens_d:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(2)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(2)  \n",
        "  if tokens_e:\n",
        "    for token in tokens_e:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(3)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(3)  \n",
        "  if tokens_f:\n",
        "    for token in tokens_f:\n",
        "      tokens.append(token)\n",
        "      segment_ids.append(4)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    segment_ids.append(4)\n",
        "    \n",
        "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  #print(len(input_ids))\n",
        "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "  # tokens are attended to.\n",
        "  input_mask = [1] * len(input_ids)\n",
        "\n",
        "  # Zero-pad up to the sequence length.\n",
        "  while len(input_ids) < max_seq_length:\n",
        "    input_ids.append(0)\n",
        "    input_mask.append(0)\n",
        "    segment_ids.append(0)\n",
        "  \n",
        "  #if len(input_ids)>128:\n",
        "    #print(tokens_a,tokens_b,tokens_c)\n",
        "  assert len(input_ids) == max_seq_length\n",
        "  assert len(input_mask) == max_seq_length\n",
        "  assert len(segment_ids) == max_seq_length\n",
        "\n",
        "  label_id = label_map[example.label]\n",
        "  if ex_index < 5:\n",
        "    tf.logging.info(\"*** Example ***\")\n",
        "    tf.logging.info(\"guid: %s\" % (example.guid))\n",
        "    tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "        [tokenization.printable_text(x) for x in tokens]))\n",
        "    tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "    tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "    tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "    tf.logging.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
        "\n",
        "  feature = InputFeatures(\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      segment_ids=segment_ids,\n",
        "      label_id=label_id,\n",
        "      is_real_example=True)\n",
        "  return feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IayTY-BYH4MM",
        "colab_type": "code",
        "outputId": "a596e4a2-a2ed-417c-d4c8-6aebe0df889a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "nput_examples[5].text_a,input_examples[5].text_b,input_examples[5].text_c,input_examples[5].text_d,input_examples[5].text_e,input_examples[5].label"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Tunisia ' s flag carrier Tunisair scored 60 million Tunisian dinars ( 41 . 15 million US dollars ) in net income in 2009 despite world economic downturn which badly hit air transport industry the company said on Friday .\",\n",
              " 'Tunisair',\n",
              " 'Tunisia',\n",
              " 'org:country_of_headquarters')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1ixz_D8FtcU",
        "colab_type": "code",
        "outputId": "63d33b0f-d509-4bdf-e769-c569a0976a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "print(input_examples[0].text_a)\n",
        "print(input_examples[0].text_b)\n",
        "print(input_examples[0].text_c)\n",
        "print(input_examples[0].text_d)\n",
        "print(input_examples[0].text_e)\n",
        "print(input_examples[0].text_f)\n",
        "print(input_examples[689].label)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\n",
            "dry palms\n",
            "wet palms\n",
            "palms covered with oil\n",
            "palms covered with lotion\n",
            "no option\n",
            "D\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KiB5HpeDPS5",
        "colab_type": "code",
        "outputId": "01150534-e588-404f-f716-37f52c724ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = convert_single_example(3, input_examples[3], get_labels(), 128,\n",
        "                           tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRq1XRb5FHCv",
        "colab_type": "code",
        "outputId": "512e34bd-9480-43ca-e2ab-103189d99a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "print(x.input_ids)\n",
        "print(x.input_mask)\n",
        "print(x.segment_ids)\n",
        "print(x.label_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 2029, 1997, 1996, 2206, 2003, 2019, 2742, 1997, 2019, 6509, 3512, 5080, 1029, 102, 3967, 10014, 102, 9055, 102, 4542, 16531, 102, 4157, 8962, 102, 2053, 5724, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_tDZK_tWPpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_based_convert_examples_to_features(\n",
        "    examples, label_list, max_seq_length, tokenizer, output_file):\n",
        "  \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
        "\n",
        "  writer = tf.python_io.TFRecordWriter(output_file)\n",
        "\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "    if ex_index % 10000 == 0:\n",
        "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "    feature = convert_single_example(ex_index, example, label_list,\n",
        "                                     max_seq_length, tokenizer)\n",
        "\n",
        "    def create_int_feature(values):\n",
        "      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
        "      return f\n",
        "\n",
        "    features = collections.OrderedDict()\n",
        "    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
        "    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
        "    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
        "    features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
        "    features[\"is_real_example\"] = create_int_feature(\n",
        "        [int(feature.is_real_example)])\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "  writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC2SsDNfWTZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
        "                                drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  name_to_features = {\n",
        "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
        "      \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
        "      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
        "  }\n",
        "\n",
        "  def _decode_record(record, name_to_features):\n",
        "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
        "    example = tf.parse_single_example(record, name_to_features)\n",
        "\n",
        "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
        "    # So cast all int64 to int32.\n",
        "    for name in list(example.keys()):\n",
        "      t = example[name]\n",
        "      if t.dtype == tf.int64:\n",
        "        t = tf.to_int32(t)\n",
        "      example[name] = t\n",
        "\n",
        "    return example\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    # For training, we want a lot of parallel reading and shuffling.\n",
        "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
        "    d = tf.data.TFRecordDataset(input_file)\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.apply(\n",
        "        tf.contrib.data.map_and_batch(\n",
        "            lambda record: _decode_record(record, name_to_features),\n",
        "            batch_size=batch_size,\n",
        "            drop_remainder=drop_remainder))\n",
        "\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9kQKGIvWg0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
        "                 labels, num_labels, use_one_hot_embeddings):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "  model = modeling.BertModel(\n",
        "      config=bert_config,\n",
        "      is_training=is_training,\n",
        "      input_ids=input_ids,\n",
        "      input_mask=input_mask,\n",
        "      token_type_ids=segment_ids,\n",
        "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "  output_layer = model.get_pooled_output()\n",
        "\n",
        "  hidden_size = output_layer.shape[-1].value\n",
        "\n",
        "  output_weights = tf.get_variable(\n",
        "      \"output_weights\", [num_labels, hidden_size],\n",
        "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "  output_bias = tf.get_variable(\n",
        "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
        "\n",
        "  with tf.variable_scope(\"loss\"):\n",
        "    if is_training:\n",
        "      # I.e., 0.1 dropout\n",
        "      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
        "\n",
        "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
        "    logits = tf.nn.bias_add(logits, output_bias)\n",
        "    probabilities = tf.nn.softmax(logits, axis=-1)\n",
        "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
        "\n",
        "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
        "    loss = tf.reduce_mean(per_example_loss)\n",
        "\n",
        "    return (loss, per_example_loss, logits, probabilities)\n",
        "\n",
        "\n",
        "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
        "                     num_train_steps, num_warmup_steps, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    tf.logging.info(\"*** Features ***\")\n",
        "    for name in sorted(features.keys()):\n",
        "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
        "\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    segment_ids = features[\"segment_ids\"]\n",
        "    label_ids = features[\"label_ids\"]\n",
        "    is_real_example = None\n",
        "    if \"is_real_example\" in features:\n",
        "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
        "    else:\n",
        "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
        "\n",
        "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
        "\n",
        "    (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
        "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
        "        num_labels, use_one_hot_embeddings)\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    initialized_variable_names = {}\n",
        "    scaffold_fn = None\n",
        "    if init_checkpoint:\n",
        "      (assignment_map, initialized_variable_names\n",
        "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
        "      if use_tpu:\n",
        "\n",
        "        def tpu_scaffold():\n",
        "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "          return tf.train.Scaffold()\n",
        "\n",
        "        scaffold_fn = tpu_scaffold\n",
        "      else:\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    tf.logging.info(\"**** Trainable Variables ****\")\n",
        "#     for var in tvars:\n",
        "#       init_string = \"\"\n",
        "#       if var.name in initialized_variable_names:\n",
        "#         init_string = \", *INIT_FROM_CKPT*\"\n",
        "#       tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "#                       init_string)\n",
        "\n",
        "    output_spec = None\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "\n",
        "      train_op = optimization.create_optimizer(\n",
        "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
        "\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          train_op=train_op,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
        "\n",
        "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
        "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
        "        accuracy = tf.metrics.accuracy(\n",
        "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
        "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
        "        return {\n",
        "            \"eval_accuracy\": accuracy,\n",
        "            \"eval_loss\": loss,\n",
        "        }\n",
        "\n",
        "      eval_metrics = (metric_fn,\n",
        "                      [per_example_loss, label_ids, logits, is_real_example])\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          loss=total_loss,\n",
        "          eval_metrics=eval_metrics,\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    else:\n",
        "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "          mode=mode,\n",
        "          predictions={\"probabilities\": probabilities},\n",
        "          scaffold_fn=scaffold_fn)\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXQJ5SfNWqCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_segment_ids = []\n",
        "  all_label_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_segment_ids.append(feature.segment_ids)\n",
        "    all_label_ids.append(feature.label_id)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "\n",
        "\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"segment_ids\":\n",
        "            tf.constant(\n",
        "                all_segment_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"label_ids\":\n",
        "            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n",
        "    })\n",
        "\n",
        "    if is_training:\n",
        "      d = d.repeat()\n",
        "      d = d.shuffle(buffer_size=100)\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVm5MMJ2WyEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
        "                                 tokenizer):\n",
        "  \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
        "\n",
        "  features = []\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "    if ex_index % 10000 == 0:\n",
        "      tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "    feature = convert_single_example(ex_index, example, label_list,\n",
        "                                     max_seq_length, tokenizer)\n",
        "\n",
        "    features.append(feature)\n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyUAOM_eW7d3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do_lower_case=True\n",
        "init_checkpoint=\"./uncased_L-12_H-768_A-12/bert_model.ckpt\"\n",
        "tokenization.validate_case_matches_checkpoint(do_lower_case,\n",
        "                                                init_checkpoint)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsVhvCSnXUOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_config_file=\"./uncased_L-12_H-768_A-12/bert_config.json\"\n",
        "bert_config = modeling.BertConfig.from_json_file(bert_config_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQ8wKAY8XgBQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_dir=\"output_dir\"\n",
        "tf.gfile.MakeDirs(output_dir)\n",
        "\n",
        "\n",
        "label_list = get_labels()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLYK09qnXv5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_file=\"./uncased_L-12_H-768_A-12/vocab.txt\"\n",
        "tokenizer = tokenization.FullTokenizer(\n",
        "      vocab_file=vocab_file, do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq6QZYEKX79z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tpu_cluster_resolver = None\n",
        "master=None\n",
        "iterations_per_loop=1000\n",
        "save_checkpoints_steps=1000\n",
        "num_tpu_cores=8\n",
        "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      master=master,\n",
        "      model_dir=output_dir,\n",
        "      save_checkpoints_steps=save_checkpoints_steps,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=iterations_per_loop,\n",
        "          num_shards=num_tpu_cores,\n",
        "          per_host_input_for_training=is_per_host))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3L4Sbu_DYSeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "do_train=True\n",
        "num_train_epochs=9\n",
        "warmup_proportion=0.1\n",
        "train_batch_size=24\n",
        "train_examples = None\n",
        "num_train_steps = None\n",
        "num_warmup_steps = None\n",
        "\n",
        "if do_train:\n",
        "    train_examples = input_examples\n",
        "    num_train_steps = int(\n",
        "        len(train_examples) / train_batch_size * num_train_epochs)\n",
        "    num_warmup_steps = int(num_train_steps * warmup_proportion)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAKWMbAxYdT-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate=1e-5\n",
        "use_tpu=False\n",
        "model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      num_labels=len(label_list),\n",
        "      init_checkpoint=init_checkpoint,\n",
        "      learning_rate=learning_rate,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      use_tpu=use_tpu,\n",
        "      use_one_hot_embeddings=use_tpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EnyUChtZC6s",
        "colab_type": "code",
        "outputId": "2900af45-4e59-40b2-96db-f8f5483305f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "eval_batch_size=train_batch_size=predict_batch_size=16\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=use_tpu,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=train_batch_size,\n",
        "      eval_batch_size=eval_batch_size,\n",
        "      predict_batch_size=predict_batch_size)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0707 17:53:07.036693 140377027782528 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fabc8e2cea0>) includes params argument, but params are not passed to Estimator.\n",
            "W0707 17:53:07.040364 140377027782528 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnuwzMdf4HzX",
        "colab_type": "text"
      },
      "source": [
        "<h2>Model training</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv4RlygPZILk",
        "colab_type": "code",
        "outputId": "f7f8b5fd-04e5-4b1e-c7ab-4273bd4b78f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "max_seq_length=170\n",
        "train_file = os.path.join(output_dir, \"train.tf_record\")\n",
        "file_based_convert_examples_to_features(\n",
        "        train_examples, label_list, max_seq_length, tokenizer, train_file)\n",
        "print(\"***** Running training *****\")\n",
        "print(\"  Num examples = %d\", len(train_examples))\n",
        "print(\"  Batch size = %d\", train_batch_size)\n",
        "print(\"  Num steps = %d\", num_train_steps)\n",
        "train_input_fn = file_based_input_fn_builder(\n",
        "        input_file=train_file,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=True,\n",
        "        drop_remainder=True)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "***** Running training *****\n",
            "  Num examples = %d 1119\n",
            "  Batch size = %d 16\n",
            "  Num steps = %d 419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfhozFouNhPk",
        "colab_type": "code",
        "outputId": "9f527e13-980a-4ae2-9470-a80c396a4e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_estimator.python.estimator.tpu.tpu_estimator.TPUEstimator at 0x7fabc88b4e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEtsUE-qOM45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mytest_file = os.path.join(output_dir, \"mytest.tf_record\")\n",
        "file_based_convert_examples_to_features(exa, label_list,\n",
        "                                            max_seq_length, tokenizer,\n",
        "                                            mytest_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeSEgIl-3ViP",
        "colab_type": "text"
      },
      "source": [
        "<h2>Evaluating the model</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDh55ObpTjhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJRvDZCriNg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_examples = input_examples_cv\n",
        "num_actual_eval_examples = len(eval_examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmvuKUuRiTku",
        "colab_type": "code",
        "outputId": "5fad9e61-9a90-442e-e8fa-aca157024ef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "eval_file = os.path.join(output_dir, \"eval.tf_record\")\n",
        "file_based_convert_examples_to_features(\n",
        "        eval_examples, label_list, max_seq_length, tokenizer, eval_file)\n",
        "\n",
        "tf.logging.info(\"***** Running evaluation *****\")\n",
        "tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n",
        "                    len(eval_examples), num_actual_eval_examples,\n",
        "                    len(eval_examples) - num_actual_eval_examples)\n",
        "tf.logging.info(\"  Batch size = %d\", eval_batch_size)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n",
            "170\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXj9KiLXip3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_input_fn = file_based_input_fn_builder(\n",
        "        input_file=eval_file,\n",
        "        seq_length=max_seq_length,\n",
        "        is_training=False,\n",
        "        drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTeSF75eZ6ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6DMn43dVuA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijwSxNzwVt6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qjzrI81i0EI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = estimator.evaluate(input_fn=eval_input_fn, steps=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8ZPXNYgjAKX",
        "colab_type": "code",
        "outputId": "0e952547-25ab-4e2a-bdff-7cac6e172869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
        "with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "      tf.logging.info(\"***** Eval results *****\")\n",
        "      for key in sorted(result.keys()):\n",
        "        print(\"  %s = %s\", key, str(result[key]))\n",
        "        print(\"%s = %s\\n\" % (key, str(result[key])))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  %s = %s eval_accuracy 0.21404682\n",
            "eval_accuracy = 0.21404682\n",
            "\n",
            "  %s = %s eval_loss 1.5966659\n",
            "eval_loss = 1.5966659\n",
            "\n",
            "  %s = %s global_step 844\n",
            "global_step = 844\n",
            "\n",
            "  %s = %s loss 1.5950656\n",
            "loss = 1.5950656\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxrFCl7R3hiI",
        "colab_type": "text"
      },
      "source": [
        "<h2>Predictions</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT1yIrM7fFLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples_test=[]\n",
        "with open(\"gdrive/My Drive/ARC-V1-Feb2018/ARC-V1-Feb2018-2/ARC-Challenge/ARC-Challenge-Test.jsonl\") as f:\n",
        "      for line in f.readlines():\n",
        "         example = json.loads(line)\n",
        "         examples_test.append(example)\n",
        "        \n",
        "input_examples_test=process_input(examples_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEWtl5vhfTpV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "750b6cec-b8a6-480b-ab49-88650701f0c9"
      },
      "source": [
        "input_examples_test[150].label"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'C'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmylHQiTNe0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_predictions(text_a,text_b,text_c,text_d,text_e,text_f,label):\n",
        "  exa=[]\n",
        "  text_a=text_a\n",
        "  text_b=text_b\n",
        "  text_c =text_c\n",
        "  text_d = text_d\n",
        "  text_e = text_e\n",
        "  text_f=text_f\n",
        "  label = label\n",
        "  exa.append(\n",
        "            InputExample(guid=1000, text_a=text_a, text_b=text_b,text_c=text_c,text_d=text_d,text_e=text_e,text_f=text_f,label=label))\n",
        "  mytest_file = os.path.join(output_dir, \"mytest.tf_record\")\n",
        "  file_based_convert_examples_to_features(exa, label_list,\n",
        "                                              max_seq_length, tokenizer,\n",
        "                                              mytest_file)\n",
        "  predict_drop_remainder=False\n",
        "  mytest_input_fn = file_based_input_fn_builder(\n",
        "          input_file=mytest_file,\n",
        "          seq_length=max_seq_length,\n",
        "          is_training=False,\n",
        "          drop_remainder=predict_drop_remainder)\n",
        "  result = estimator.predict(input_fn=mytest_input_fn)\n",
        "  for (i, prediction) in enumerate(result):\n",
        "          #tf.logging.info(i)\n",
        "          print(\"************************probability*********************************\",prediction[\"probabilities\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "07331512-ad2c-4fae-c035-b2fdc75fc657",
        "id": "PJxHRzhvhwB1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "source": [
        "import numpy as np\n",
        "outputs = []\n",
        "for i in range(10):\n",
        "  \n",
        "  text_a=input_examples_test[i].text_a\n",
        "\n",
        "  text_b=input_examples_test[i].text_b\n",
        "\n",
        "  text_c = input_examples_test[i].text_c\n",
        "\n",
        "  text_d = input_examples_test[i].text_d\n",
        "\n",
        "  text_e = input_examples_test[i].text_e\n",
        "\n",
        "  text_f = input_examples_test[i].text_f\n",
        "\n",
        "  label = input_examples_test[i].label\n",
        "  \n",
        "  predictions_prob = get_predictions(text_a,text_b,text_c,text_d,text_e,text_f,label)\n",
        "  np.argmax(predictions_prob)\n",
        "  np.max(predictions_prob)\n",
        "  outputs.append(np.argmax(predictions_prob))\n",
        "  print(label)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "170\n",
            "************************probability********************************* [0.26003313 0.31086117 0.14544459 0.27889854 0.00476258]\n",
            "C\n",
            "170\n",
            "************************probability********************************* [0.19258508 0.10781594 0.12901746 0.56655335 0.00402817]\n",
            "B\n",
            "170\n",
            "************************probability********************************* [0.28862333 0.3127576  0.32073018 0.07141327 0.00647562]\n",
            "C\n",
            "170\n",
            "************************probability********************************* [0.11551436 0.09069177 0.06005534 0.721886   0.01185256]\n",
            "D\n",
            "170\n",
            "************************probability********************************* [0.14188863 0.46147552 0.16906597 0.2192921  0.00827784]\n",
            "D\n",
            "170\n",
            "************************probability********************************* [0.15847743 0.44914412 0.27993557 0.09721778 0.01522514]\n",
            "B\n",
            "170\n",
            "************************probability********************************* [0.21924129 0.3274577  0.29183823 0.15523866 0.00622406]\n",
            "C\n",
            "170\n",
            "************************probability********************************* [0.19820704 0.34609908 0.3600008  0.08660091 0.00909214]\n",
            "C\n",
            "170\n",
            "************************probability********************************* [0.18594037 0.35842535 0.35912228 0.08224091 0.01427105]\n",
            "B\n",
            "170\n",
            "************************probability********************************* [0.1401849  0.47250164 0.26813063 0.10611583 0.01306697]\n",
            "A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWI1XD-rieUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a81d2d4e-c7c7-4596-b170-4b66c2126153"
      },
      "source": [
        "outputs"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYyfD6ofjMl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}